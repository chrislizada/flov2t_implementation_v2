# FLoV2T Quick Start Guide

## üéØ What You Have

A **60% complete** implementation of FLoV2T with all core components working:
- ‚úÖ Packet2Patch transformation
- ‚úÖ LoRA (98.44% parameter reduction)
- ‚úÖ ViT-based RTFE module
- ‚úÖ RGPA aggregation
- ‚úÖ Federated data splitting

## üìã File Overview

```
16 files created in:
flov2t/
```

| Category | Files | Status |
|----------|-------|--------|
| **Documentation** | README.md, INSTALLATION.md, PROJECT_STATUS.md, IMPLEMENTATION_SUMMARY.md, QUICK_START.md | ‚úÖ Complete |
| **Config** | config.yaml, model_config.py | ‚úÖ Complete |
| **Data** | packet2patch.py, dataset.py, data_loader.py | ‚úÖ Complete |
| **Models** | lora.py, rtfe.py | ‚úÖ Complete |
| **Federated** | aggregation.py | ‚úÖ Complete |
| **Utils** | metrics.py, logger.py, visualization.py | ‚è≥ To create |
| **Scripts** | train.py, evaluate.py, preprocess_cicids.py | ‚è≥ To create |

## üöÄ Quick Test (Components)

Test individual components to verify they work:

```bash
cd flov2t

# 1. Test LoRA
python -m models.lora
# Expected output: Parameter counts, forward pass test

# 2. Test RTFE  
python -m models.rtfe
# Expected output: Model loading, parameter extraction

# 3. Test Aggregation
python -m federated.aggregation
# Expected output: RGPA vs FedAvg comparison

# 4. Test Packet2Patch
python -m data.packet2patch
# Expected output: Transformer initialization
```

## üì• What You Need To Add

### Priority 1: Training Pipeline (Critical)

Create `federated/client.py`:
```python
class FLoV2TClient:
    def __init__(self, client_id, model, dataloader, device):
        # Initialize client
        
    def train(self, epochs=1, lr=1e-4):
        # Local training
        # Return LoRA parameters
```

Create `federated/server.py`:
```python
class RGPAServer:
    def __init__(self, global_model, lambda_reg=0.1):
        # Initialize server
        
    def aggregate(self, client_params, client_weights):
        # Use rgpa_aggregate from aggregation.py
        
    def update_global_model(self, aggregated_params):
        # Update model
```

Create `train.py`:
```python
# Main training loop
# Load config ‚Üí Create clients ‚Üí Train for N rounds ‚Üí Save checkpoints
```

### Priority 2: Data Preparation

`preprocess_cicids.py` extracts specific attack flows using CSV labels:
```python
# Map CSV labels to attack types (Bot ‚Üí Botnet, SSH-Patator ‚Üí SSH-BruteForce)
# Extract flows by matching Source IP + Destination IP
# Convert to images using Packet2PatchTransformer
# Save ~9K flows total (per paper Table 1)
```

### Priority 3: Evaluation

Create `evaluate.py`:
```python
# Load checkpoint
# Run on test set
# Calculate metrics
# Generate confusion matrix
```

## üìä Testing Strategy

### Step 1: Unit Tests (Already Possible)
```bash
# Test each component individually
python -m models.lora
python -m models.rtfe
python -m federated.aggregation
```

### Step 2: Integration Test (After completing scripts)
```bash
# Test with a single CSV-PCAP pair
python preprocess_cicids.py \
    --pcap-dir test_data/raw/ \
    --csv-dir test_data/csv/ \
    --output test_processed/
python train.py --config config/config.yaml --num_clients 2 --rounds 1
```

### Step 3: Full Training
```bash
# Full CICIDS2017, 3 clients, 18 rounds
python train.py --config config/config.yaml --num_clients 3 --distribution non_iid --rounds 18
```

## üí° Key Design Decisions

### Why This Structure?

1. **Modular Design**: Each component is independent
   - `data/`: Handles all data processing
   - `models/`: Contains model architectures
   - `federated/`: FL-specific logic
   - Easy to test and debug

2. **Paper Alignment**: Follows FLoV2T exactly
   - Packet2Patch: 20B + 20B + 216B structure
   - LoRA: r=4, Œ±=8
   - RGPA: Œª=0.1
   - Non-IID: Table 2 configurations

3. **Extensibility**: Easy to modify
   - Change config.yaml for different settings
   - Swap aggregation methods
   - Add new datasets

## üîß Common Tasks

### Change Number of Clients
Edit `config/config.yaml`:
```yaml
federated:
  num_clients: 5  # Change from 3 to 5
```

### Use IID Distribution
Edit `config/config.yaml`:
```yaml
federated:
  distribution: "iid"  # Change from "non_iid"
```

### Modify LoRA Rank
Edit `config/config.yaml`:
```yaml
lora:
  rank: 8  # Change from 4
  alpha: 16  # Change from 8
```

### Use CPU Instead of GPU
Edit `config/config.yaml`:
```yaml
hardware:
  device: "cpu"  # Change from "cuda"
```

## üì¶ What Each File Does

### Data Processing
- **packet2patch.py**: PCAP ‚Üí Image (core transformation)
- **dataset.py**: PyTorch Dataset wrapper
- **data_loader.py**: Splits data for federated learning

### Models
- **lora.py**: Low-rank adaptation layers
- **rtfe.py**: ViT with LoRA (main model)

### Federated Learning
- **aggregation.py**: RGPA algorithm implementation

### Configuration
- **config.yaml**: All hyperparameters
- **model_config.py**: Model-specific settings

## üéì Learning Path

### Understand the Paper
1. Read IMPLEMENTATION_SUMMARY.md
2. Check config.yaml for hyperparameters
3. Review paper methodology section

### Understand the Code
1. Start with `models/lora.py` - Simple LoRA implementation
2. Then `data/packet2patch.py` - See data transformation
3. Then `models/rtfe.py` - See how LoRA integrates with ViT
4. Then `federated/aggregation.py` - See RGPA algorithm

### Run Experiments (After completing scripts)
1. Small test: 2 clients, 2 rounds, 100 samples
2. Medium test: 3 clients, 5 rounds, 1000 samples
3. Full experiment: 5 clients, 18 rounds, full dataset

## ‚ö†Ô∏è Known Limitations

1. **No training script yet** - Need to create `train.py`
2. **No preprocessing yet** - Need to create `preprocess_cicids.py`
3. **No evaluation yet** - Need to create `evaluate.py`
4. **Requires CICIDS2017** - Must download separately

## üèÅ Next Actions

### For You (User)
1. ‚úÖ Review the created files
2. ‚è≥ Install dependencies: `pip install -r requirements.txt`
3. ‚è≥ Download CICIDS2017 dataset
4. ‚è≥ Decide: Complete the implementation yourself OR request remaining components

### For Complete Implementation
1. Create client/server classes (2-3 hours work)
2. Create training script (2-3 hours work)
3. Create preprocessing script (1-2 hours work)
4. Create evaluation script (1 hour work)
5. Test and debug (variable time)

## üìû Support Files

- **Stuck?** ‚Üí Read INSTALLATION.md
- **Want details?** ‚Üí Read IMPLEMENTATION_SUMMARY.md  
- **Check progress?** ‚Üí Read PROJECT_STATUS.md
- **Start using?** ‚Üí Read README.md

## üéâ You're Ready When...

‚úÖ All files reviewed  
‚úÖ Dependencies installed  
‚úÖ Dataset downloaded  
‚úÖ Components tested  
‚è≥ Training script created  
‚è≥ First experiment run  

---

**Current Status**: Foundation complete, ready for integration!  
**Estimated time to complete**: 6-8 hours of focused work  
**Difficulty**: Moderate (requires PyTorch + FL knowledge)
